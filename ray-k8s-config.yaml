# This config was pulled largely from the demo for KubeRay's Ray autoscaler integration
apiVersion: ray.io/v1alpha1
kind: RayCluster
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
    # A unique identifier for the head node and workers of this cluster.
  name: hparam-cluster
spec:
  rayVersion: '2.2.0'
  enableInTreeAutoscaling: False


    # Ray head pod template
  headGroupSpec:
    serviceType: ClusterIP

    rayStartParams:
      dashboard-host: '0.0.0.0'
      block: 'true'
      
    template:
      spec:
        containers:
        - name: ray-head
          image: apalermo02/hparam_image:latest
          imagePullPolicy: Never
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "ray stop"]
          
          # increase these resources for production
          resources:
            limits:
              cpu: "1"
              memory: "2G"
            requests:
              cpu: "500m"
              memory: "2G"
  workerGroupSpecs:
  - replicas: 1
    minReplicas: 1
    maxReplicas: 10
    groupName: small-group

    rayStartParams:
      block: 'true'

    # pod template
    template:
      spec:
        initContainers:

        - name: init
          image: busybox:1.28
          command: ['sh', '-c', "until nslookup $RAY_IP.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for K8s Service $RAY_IP; sleep 2; done"]
        containers:
        - name: ray-worker
          image: hparam_image:latest
          imagePullPolicy: Never

          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","ray stop"]
          # The resource requests and limits in this config are too small for production!
          # For an example with more realistic resource configuration, see
          # ray-cluster.autoscaler.large.yaml.
          # It is better to use a few large Ray pod than many small ones.
          # For production, it is ideal to size each Ray pod to take up the
          # entire Kubernetes node on which it is scheduled.
          resources:
            limits:
              cpu: "1"
              memory: "1G"
            # For production use-cases, we recommend specifying integer CPU reqests and limits.
            # We also recommend setting requests equal to limits for both CPU and memory.
            # For this example, we use a 500m CPU request to accomodate resource-constrained local
            # Kubernetes testing environments such as KinD and minikube.
            requests:
              cpu: "500m"
              memory: "1G"
