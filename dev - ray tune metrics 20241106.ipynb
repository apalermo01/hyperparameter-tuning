{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b9de96-d02c-4b7e-9c92-fe3903f0a22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-07 07:07:51</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:10.63        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.9/15.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=10<br>Bracket: Iter 8.000: 0.013768627308309078 | Iter 4.000: 0.01888203714042902 | Iter 2.000: 0.026161392219364643 | Iter 1.000: 0.038130491971969604<br>Logical resource usage: 2.0/4 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">            train_loop_config/op\n",
       "timizer_cfg/args/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  val_loss_epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_7771b_00000</td><td>TERMINATED</td><td>192.168.1.242:20825</td><td style=\"text-align: right;\">0.00271679 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         17.1354</td><td style=\"text-align: right;\">   0.027331 </td><td style=\"text-align: right;\"> 0.0218844</td><td style=\"text-align: right;\">       0.0218844</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00001</td><td>TERMINATED</td><td>192.168.1.242:20826</td><td style=\"text-align: right;\">1.37265e-06</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         58.0962</td><td style=\"text-align: right;\">   0.0386087</td><td style=\"text-align: right;\"> 0.0194347</td><td style=\"text-align: right;\">       0.0194347</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00002</td><td>TERMINATED</td><td>192.168.1.242:21343</td><td style=\"text-align: right;\">3.47354e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.5652</td><td style=\"text-align: right;\">   0.0174068</td><td style=\"text-align: right;\"> 0.0184312</td><td style=\"text-align: right;\">       0.0184312</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00003</td><td>TERMINATED</td><td>192.168.1.242:21962</td><td style=\"text-align: right;\">0.0427873  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         11.4396</td><td style=\"text-align: right;\">   0.0260426</td><td style=\"text-align: right;\"> 0.0376868</td><td style=\"text-align: right;\">       0.0376868</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00004</td><td>TERMINATED</td><td>192.168.1.242:22106</td><td style=\"text-align: right;\">4.04176e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.0545</td><td style=\"text-align: right;\">   0.0316389</td><td style=\"text-align: right;\"> 0.0360793</td><td style=\"text-align: right;\">       0.0360793</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00005</td><td>TERMINATED</td><td>192.168.1.242:22434</td><td style=\"text-align: right;\">0.000552889</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.3999</td><td style=\"text-align: right;\">   0.0292648</td><td style=\"text-align: right;\"> 0.0354182</td><td style=\"text-align: right;\">       0.0354182</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00006</td><td>TERMINATED</td><td>192.168.1.242:22495</td><td style=\"text-align: right;\">0.00917663 </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         51.464 </td><td style=\"text-align: right;\">   0.0221682</td><td style=\"text-align: right;\"> 0.013477 </td><td style=\"text-align: right;\">       0.013477 </td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00007</td><td>TERMINATED</td><td>192.168.1.242:22789</td><td style=\"text-align: right;\">0.081194   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.3244</td><td style=\"text-align: right;\">   0.0255141</td><td style=\"text-align: right;\"> 0.0249089</td><td style=\"text-align: right;\">       0.0249089</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00008</td><td>TERMINATED</td><td>192.168.1.242:23293</td><td style=\"text-align: right;\">5.14895e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.2265</td><td style=\"text-align: right;\">   0.020091 </td><td style=\"text-align: right;\"> 0.0373921</td><td style=\"text-align: right;\">       0.0373921</td></tr>\n",
       "<tr><td>TorchTrainer_7771b_00009</td><td>TERMINATED</td><td>192.168.1.242:23492</td><td style=\"text-align: right;\">0.0092704  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         36.6822</td><td style=\"text-align: right;\">   0.0157573</td><td style=\"text-align: right;\"> 0.0137686</td><td style=\"text-align: right;\">       0.0137686</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=20825)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20825)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=20979) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m pl version =  2.4.0\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m model is LightningModule? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00000_0_lr=0.0027_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=20826)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=20826)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=20985) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m ----------------------------------------------------\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20979)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00000_0_lr=0.0027_2024-11-07_07-04-40/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00001_1_lr=0.0000_2024-11-07_07-04-40/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=21343)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21343)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=21427) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00001_1_lr=0.0000_2024-11-07_07-04-40/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00001_1_lr=0.0000_2024-11-07_07-04-40/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m pl version =  2.4.0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m model is LightningModule? True\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00001_1_lr=0.0000_2024-11-07_07-04-40/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00002_2_lr=0.0000_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00001_1_lr=0.0000_2024-11-07_07-04-40/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00002_2_lr=0.0000_2024-11-07_07-04-40/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=21427)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00002_2_lr=0.0000_2024-11-07_07-04-40/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[36m(TorchTrainer pid=21962)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=21962)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=22188) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=20985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00001_1_lr=0.0000_2024-11-07_07-04-40/checkpoint_000009)\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m pl version =  2.4.0\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m model is LightningModule? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m pl version =  2.4.0\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m model is LightningModule? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22188)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00003_3_lr=0.0428_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=22106)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22106)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=22228) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m ----------------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(TorchTrainer pid=22434)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22434)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=22539) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00004_4_lr=0.0000_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m pl version =  2.4.0\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m model is LightningModule? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m pl version =  2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22539)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00005_5_lr=0.0006_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=22495)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22495)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=22580) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m ----------------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000002)\n",
      "\u001b[36m(TorchTrainer pid=22789)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=22789)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=22920) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m model is LightningModule? True\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m pl version =  2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00007_7_lr=0.0812_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=22920)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00007_7_lr=0.0812_2024-11-07_07-04-40/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=22580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00006_6_lr=0.0092_2024-11-07_07-04-40/checkpoint_000007)\n",
      "\u001b[36m(TorchTrainer pid=23293)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23293)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=23389) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m pl version =  2.4.0\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m model is LightningModule? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=23389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00008_8_lr=0.0001_2024-11-07_07-04-40/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=23492)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=23492)\u001b[0m - (node_id=cc851b59aea9d2f70032b2b12fb0ac0b5cab7dcae4d7620ae6bad226, ip=192.168.1.242, pid=23587) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m pl version =  2.4.0\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m model is LightningModule? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m   | Name  | Type              | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 0 | model | Net               | 85.9 K | train\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m ----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 85.9 K    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 85.9 K    Total params\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 0.343     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 20        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m /home/alex/Documents/github/hyperparameter-tuning/env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000000)\n",
      "2024-11-07 07:07:28,411\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000006)\n",
      "2024-11-07 07:07:51,003\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35' in 0.0057s.\n",
      "2024-11-07 07:07:51,018\tINFO tune.py:1041 -- Total run time: 190.92 seconds (190.62 seconds for the tuning loop).\n",
      "\u001b[36m(RayTrainWorker pid=23587)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/alex/ray_results/TorchTrainer_2024-11-07_07-04-35/TorchTrainer_7771b_00009_9_lr=0.0093_2024-11-07_07-04-40/checkpoint_000007)\n"
     ]
    }
   ],
   "source": [
    "from hparam_tuning_project.optimization_modules.learning_rate_pl import tune_lr\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "def main(cfg):\n",
    "\n",
    "    search_space = {\n",
    "        'optimizer_cfg': {\n",
    "            'args': {\n",
    "                'lr': tune.loguniform(1e-6, 1e-1)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    tuner = tune_lr(cfg=cfg, num_samples=10, search_space=search_space)\n",
    "    return tuner\n",
    "    # return {\n",
    "    #     'model_id': cfg['model_cfg']['model_id'],\n",
    "    #     'split_perc': split,\n",
    "    #     'best_lrs': best_lrs\n",
    "    # }\n",
    "\n",
    "with open(os.path.join(\"./experiments/opt_single_lr/training_configs/\", \"pytorch_classifier_mnist_10.yaml\"), \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "tuner = main(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60afd0d9-86eb-43b5-89d6-5e89b057a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': {0: 0.027331000193953514,\n",
       "  1: 0.03860867768526077,\n",
       "  2: 0.017406780272722244,\n",
       "  3: 0.026042645797133446,\n",
       "  4: 0.03163888305425644,\n",
       "  5: 0.029264766722917557,\n",
       "  6: 0.022168200463056564,\n",
       "  7: 0.02551409602165222,\n",
       "  8: 0.020090987905859947,\n",
       "  9: 0.015757322311401367},\n",
       " 'val_loss': {0: 0.021884404122829437,\n",
       "  1: 0.019434720277786255,\n",
       "  2: 0.018431218340992928,\n",
       "  3: 0.037686776369810104,\n",
       "  4: 0.0360792875289917,\n",
       "  5: 0.03541821613907814,\n",
       "  6: 0.013477005995810032,\n",
       "  7: 0.02490892820060253,\n",
       "  8: 0.037392083555459976,\n",
       "  9: 0.013768627308309078},\n",
       " 'val_loss_epoch': {0: 0.021884404122829437,\n",
       "  1: 0.019434720277786255,\n",
       "  2: 0.018431218340992928,\n",
       "  3: 0.037686776369810104,\n",
       "  4: 0.0360792875289917,\n",
       "  5: 0.03541821613907814,\n",
       "  6: 0.013477005995810032,\n",
       "  7: 0.02490892820060253,\n",
       "  8: 0.037392083555459976,\n",
       "  9: 0.013768627308309078},\n",
       " 'epoch': {0: 1, 1: 9, 2: 3, 3: 0, 4: 0, 5: 0, 6: 7, 7: 1, 8: 0, 9: 7},\n",
       " 'step': {0: 600,\n",
       "  1: 3000,\n",
       "  2: 1200,\n",
       "  3: 300,\n",
       "  4: 300,\n",
       "  5: 300,\n",
       "  6: 2400,\n",
       "  7: 600,\n",
       "  8: 300,\n",
       "  9: 2400},\n",
       " 'timestamp': {0: 1730981104,\n",
       "  1: 1730981145,\n",
       "  2: 1730981142,\n",
       "  3: 1730981163,\n",
       "  4: 1730981166,\n",
       "  5: 1730981185,\n",
       "  6: 1730981225,\n",
       "  7: 1730981214,\n",
       "  8: 1730981237,\n",
       "  9: 1730981270},\n",
       " 'checkpoint_dir_name': {0: 'checkpoint_000001',\n",
       "  1: 'checkpoint_000009',\n",
       "  2: 'checkpoint_000003',\n",
       "  3: 'checkpoint_000000',\n",
       "  4: 'checkpoint_000000',\n",
       "  5: 'checkpoint_000000',\n",
       "  6: 'checkpoint_000007',\n",
       "  7: 'checkpoint_000001',\n",
       "  8: 'checkpoint_000000',\n",
       "  9: 'checkpoint_000007'},\n",
       " 'should_checkpoint': {0: True,\n",
       "  1: True,\n",
       "  2: True,\n",
       "  3: True,\n",
       "  4: True,\n",
       "  5: True,\n",
       "  6: True,\n",
       "  7: True,\n",
       "  8: True,\n",
       "  9: True},\n",
       " 'done': {0: True,\n",
       "  1: True,\n",
       "  2: True,\n",
       "  3: True,\n",
       "  4: True,\n",
       "  5: True,\n",
       "  6: True,\n",
       "  7: True,\n",
       "  8: True,\n",
       "  9: True},\n",
       " 'training_iteration': {0: 2,\n",
       "  1: 10,\n",
       "  2: 4,\n",
       "  3: 1,\n",
       "  4: 1,\n",
       "  5: 1,\n",
       "  6: 8,\n",
       "  7: 2,\n",
       "  8: 1,\n",
       "  9: 8},\n",
       " 'trial_id': {0: '7771b_00000',\n",
       "  1: '7771b_00001',\n",
       "  2: '7771b_00002',\n",
       "  3: '7771b_00003',\n",
       "  4: '7771b_00004',\n",
       "  5: '7771b_00005',\n",
       "  6: '7771b_00006',\n",
       "  7: '7771b_00007',\n",
       "  8: '7771b_00008',\n",
       "  9: '7771b_00009'},\n",
       " 'date': {0: '2024-11-07_07-05-04',\n",
       "  1: '2024-11-07_07-05-45',\n",
       "  2: '2024-11-07_07-05-42',\n",
       "  3: '2024-11-07_07-06-03',\n",
       "  4: '2024-11-07_07-06-06',\n",
       "  5: '2024-11-07_07-06-25',\n",
       "  6: '2024-11-07_07-07-05',\n",
       "  7: '2024-11-07_07-06-54',\n",
       "  8: '2024-11-07_07-07-17',\n",
       "  9: '2024-11-07_07-07-50'},\n",
       " 'time_this_iter_s': {0: 5.0548975467681885,\n",
       "  1: 6.021053314208984,\n",
       "  2: 5.118803262710571,\n",
       "  3: 11.439555406570435,\n",
       "  4: 12.054541110992432,\n",
       "  5: 13.399857521057129,\n",
       "  6: 5.636086702346802,\n",
       "  7: 5.237184524536133,\n",
       "  8: 12.226525068283081,\n",
       "  9: 3.776461601257324},\n",
       " 'time_total_s': {0: 17.13536310195923,\n",
       "  1: 58.09623599052429,\n",
       "  2: 28.56520676612854,\n",
       "  3: 11.439555406570435,\n",
       "  4: 12.054541110992432,\n",
       "  5: 13.399857521057129,\n",
       "  6: 51.46404814720154,\n",
       "  7: 19.324373483657837,\n",
       "  8: 12.226525068283081,\n",
       "  9: 36.6821551322937},\n",
       " 'pid': {0: 20825,\n",
       "  1: 20826,\n",
       "  2: 21343,\n",
       "  3: 21962,\n",
       "  4: 22106,\n",
       "  5: 22434,\n",
       "  6: 22495,\n",
       "  7: 22789,\n",
       "  8: 23293,\n",
       "  9: 23492},\n",
       " 'hostname': {0: 'eddie',\n",
       "  1: 'eddie',\n",
       "  2: 'eddie',\n",
       "  3: 'eddie',\n",
       "  4: 'eddie',\n",
       "  5: 'eddie',\n",
       "  6: 'eddie',\n",
       "  7: 'eddie',\n",
       "  8: 'eddie',\n",
       "  9: 'eddie'},\n",
       " 'node_ip': {0: '192.168.1.242',\n",
       "  1: '192.168.1.242',\n",
       "  2: '192.168.1.242',\n",
       "  3: '192.168.1.242',\n",
       "  4: '192.168.1.242',\n",
       "  5: '192.168.1.242',\n",
       "  6: '192.168.1.242',\n",
       "  7: '192.168.1.242',\n",
       "  8: '192.168.1.242',\n",
       "  9: '192.168.1.242'},\n",
       " 'time_since_restore': {0: 17.13536310195923,\n",
       "  1: 58.09623599052429,\n",
       "  2: 28.56520676612854,\n",
       "  3: 11.439555406570435,\n",
       "  4: 12.054541110992432,\n",
       "  5: 13.399857521057129,\n",
       "  6: 51.46404814720154,\n",
       "  7: 19.324373483657837,\n",
       "  8: 12.226525068283081,\n",
       "  9: 36.6821551322937},\n",
       " 'iterations_since_restore': {0: 2,\n",
       "  1: 10,\n",
       "  2: 4,\n",
       "  3: 1,\n",
       "  4: 1,\n",
       "  5: 1,\n",
       "  6: 8,\n",
       "  7: 2,\n",
       "  8: 1,\n",
       "  9: 8},\n",
       " 'config/train_loop_config/optimizer_cfg/args/lr': {0: 0.002716787716150458,\n",
       "  1: 1.3726468075352114e-06,\n",
       "  2: 3.4735403936543158e-06,\n",
       "  3: 0.042787277942908944,\n",
       "  4: 4.041755276333632e-05,\n",
       "  5: 0.0005528886315694084,\n",
       "  6: 0.009176627771683616,\n",
       "  7: 0.08119400679660568,\n",
       "  8: 5.14895057241253e-05,\n",
       "  9: 0.009270395336031337},\n",
       " 'logdir': {0: '7771b_00000',\n",
       "  1: '7771b_00001',\n",
       "  2: '7771b_00002',\n",
       "  3: '7771b_00003',\n",
       "  4: '7771b_00004',\n",
       "  5: '7771b_00005',\n",
       "  6: '7771b_00006',\n",
       "  7: '7771b_00007',\n",
       "  8: '7771b_00008',\n",
       "  9: '7771b_00009'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_results().get_dataframe().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923f0be0-85f3-4466-8d67-3b9eade337fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.026042645797133446,\n",
       " 'val_loss': 0.037686776369810104,\n",
       " 'val_loss_epoch': 0.037686776369810104,\n",
       " 'epoch': 0,\n",
       " 'step': 300,\n",
       " 'timestamp': 1730981163,\n",
       " 'checkpoint_dir_name': 'checkpoint_000000',\n",
       " 'should_checkpoint': True,\n",
       " 'done': True,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': '7771b_00003',\n",
       " 'date': '2024-11-07_07-06-03',\n",
       " 'time_this_iter_s': 11.439555406570435,\n",
       " 'time_total_s': 11.439555406570435,\n",
       " 'pid': 21962,\n",
       " 'hostname': 'eddie',\n",
       " 'node_ip': '192.168.1.242',\n",
       " 'config': {'train_loop_config': {'optimizer_cfg': {'args': {'lr': 0.042787277942908944}}}},\n",
       " 'time_since_restore': 11.439555406570435,\n",
       " 'iterations_since_restore': 1,\n",
       " 'experiment_tag': '3_lr=0.0428'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_results().get_best_result().metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff57298-f68a-4fa4-b831-d1441644f74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
